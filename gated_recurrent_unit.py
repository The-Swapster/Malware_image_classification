import tensorflow as tf 
from tensorflow import keras 
from keras.models import Sequential 
from keras.layers import Dense, Flatten, Dropout, GRU 
from tensorflow.keras import layers 
from keras.utils import to_categorical 
from keras.layers import LeakyReLU

# building the model
model = Sequential()
model.add(GRU(256, return_sequences=True, input_shape=(32,32)))
model.add(LeakyReLU(alpha=0.1))
model.add(GRU(256, return_sequences=True))
model.add(LeakyReLU(alpha=0.1))
model.add(GRU(256, return_sequences=True))
model.add(LeakyReLU(alpha=0.1))
model.add(GRU(256, return_sequences=True))
model.add(LeakyReLU(alpha=0.1))
model.add(GRU(256, return_sequences=True))
model.add(LeakyReLU(alpha=0.1))
model.add(Dropout(0.85))
model.add(Flatten())
model.add(Dense(25, activation = 'softmax'))

# compiling and printing the summary of model built
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.summary()

# fitting the model and evaluating
hist = model.fit(x_train, y_train, batch_size = 256, epochs = 100, validation_split = 0.1)
model.evaluate(x_test, y_test)[1]

# printing the testing loss and accuracy of model
print("Loss of the model is - " , model.evaluate(x_test,y_test)[0])
print("Accuracy of the model is - " , model.evaluate(x_test,y_test)[1]*100 , "%")

# getting average accuracies and losses
train_acc = hist.history['accuracy']
train_loss = hist.history['loss']
val_acc = hist.history['val_accuracy']
val_loss = hist.history['val_loss']
print('Average training accuracy: ', np.mean(train_acc))
print('Average training loss: ', np.mean(train_loss))
print('Average validation accuracy: ', np.mean(val_acc))
print('Average validation loss: ', np.mean(val_acc))

# plotting the training and testing accuracy and loss
epochs = [i for i in range(100)]
fig , ax = plt.subplots(1,2)
fig.set_size_inches(20,10)

ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')
ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')
ax[0].set_title('Training & Validation Accuracy')
ax[0].legend()
ax[0].set_xlabel("Epochs")
ax[0].set_ylabel("Accuracy")

ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')
ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')
ax[1].set_title('Testing Accuracy & Loss')
ax[1].legend()
ax[1].set_xlabel("Epochs")
ax[1].set_ylabel("Training & Validation Loss")
plt.show()

# predicting the images
y_pred = model.predict_classes(x_test)
print(y_pred)
rounded_labels=np.argmax(y_test, axis=1)
print(rounded_labels)

from sklearn import metrics

# evaluating model
print("Accuracy:",metrics.accuracy_score(rounded_labels, y_pred))
print("Precision:", metrics.precision_score(rounded_labels, y_pred,pos_label='positive', average='micro'))
print("Recall:", metrics.recall_score(rounded_labels, y_pred,pos_label='positive', average='micro'))
def specificity_score(y_true, y_pred):
    p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred,pos_label='positive', average='micro')
    return r[0]
print("sensitivity:", metrics.recall_score(rounded_labels, y_pred,pos_label='positive', average='micro'))
#print("specificity:", specificity_score(rounded_labels, y_pred))
print("f1 score:", metrics.f1_score(rounded_labels, y_pred,pos_label='positive', average='micro'))

# printing the classification report
print(metrics.classification_report(rounded_labels, y_pred, target_names = label))

import seaborn as sns
import matplotlib.pyplot as plt

# printing the confusion matrix
fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(metrics.confusion_matrix(rounded_labels, y_pred), annot=True, ax=ax)

# reshaping the predictions
yr_pred = y_pred.reshape(1870,1)
yr_pred.shape

# declaring variables
fpr = {}
tpr = {}
thresh ={}

# getting the roc_auc score
metrics.roc_auc_score(rounded_labels.reshape(1870,1), model.predict_proba(x_test), multi_class='ovr')

# printing the roc curve
for i in range(25):    
    fpr[i], tpr[i], thresh[i] = metrics.roc_curve(rounded_labels, y_pred, pos_label=i)

fig, ax = plt.subplots(figsize=(20,20))
for i in range(25):
  sns.lineplot(fpr[i], tpr[i], linestyle='--', label=f'{label[i]} vs Rest', ax=ax)
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
#plt.savefig('Multiclass ROC',dpi=300);

from sklearn.model_selection import KFold

# reshaping the data array
data1 = data.reshape(-1, 32, 32)
classes_cat1 = np.array(classes_cat)

# declaring variables
acc_per_fold = []
loss_per_fold = []

# kfold crossvalidating the model and evaluating the model
kfold = KFold(n_splits=10, shuffle=True)
fold_no = 1
for train, test in kfold.split(data1, classes_cat1):
  model = Sequential()
  model.add(GRU(256, return_sequences=True, input_shape=(32,32)))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(Dropout(0.85))
  model.add(Flatten())
  model.add(Dense(25, activation = 'softmax'))

  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  hist = model.fit(data1[train], classes_cat1[train], batch_size = 256, epochs = 300, validation_split = 0.1)
  model.evaluate(x_test, y_test)[1]

  scores = model.evaluate(data1[test], classes_cat1[test], verbose=0)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  # Increase fold number
  fold_no = fold_no + 1

print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

acc_per_fold = []
loss_per_fold = []
kfold = KFold(n_splits=10, shuffle=True)
fold_no = 1
for train, test in kfold.split(data1, classes_cat1):
  model = Sequential()
  model.add(GRU(256, return_sequences=True, input_shape=(32,32)))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(GRU(256, return_sequences=True))
  model.add(LeakyReLU(alpha=0.1))
  model.add(Dropout(0.85))
  model.add(Flatten())
  model.add(Dense(25, activation = 'softmax'))

  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  hist = model.fit(data1[train], classes_cat1[train], batch_size = 256, epochs = 500, validation_split = 0.1)
  model.evaluate(x_test, y_test)[1]

  scores = model.evaluate(data1[test], classes_cat1[test], verbose=0)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  # Increase fold number
  fold_no = fold_no + 1

print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')
